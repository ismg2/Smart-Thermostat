#!/usr/bin/env python3
"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  stedgeai_benchmark.py â€” Smart Thermostat Edge AI            â”‚
â”‚  Benchmark interactif via ST Edge AI Developer Cloud         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Usage :                                                     â”‚
â”‚    python stedgeai_benchmark.py                              â”‚
â”‚    python stedgeai_benchmark.py --model mon_modele.tflite    â”‚
â”‚    python stedgeai_benchmark.py --model m.tflite --board ... â”‚
â”‚    python stedgeai_benchmark.py --no-benchmark               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DÃ©pendances SDK (Ã  faire une fois) :                        â”‚
â”‚    git clone https://github.com/STMicroelectronics/          â”‚
â”‚              stm32ai-modelzoo-services.git                   â”‚
â”‚    pip install -r stm32ai-modelzoo-services/requirements.txt â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Variables d'env (optionnelles) :                            â”‚
â”‚    STEDGEAI_USERNAME   â†’ email myST                          â”‚
â”‚    STEDGEAI_PASSWORD   â†’ mot de passe myST                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

import sys
import os
import argparse
import getpass
import json
import importlib
import traceback
from pathlib import Path
from datetime import datetime


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”‘ IDENTIFIANTS â€” Ã€ remplir une bonne fois pour toutes
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
#  Remplis juste les deux lignes ci-dessous et tu n'auras plus
#  jamais Ã  les retaper. Le script les lit en prioritÃ©.
#
#  âš ï¸  Ne committe pas ce fichier avec tes vrais identifiants !
#      (ajoute stedgeai_benchmark.py Ã  ton .gitignore si besoin)
#
STEDGEAI_USERNAME = "ismail.guedira@gmail.com"   # â† ton email myST  (ex: "prenom.nom@gmail.com")
STEDGEAI_PASSWORD = "LAxdWDhk22#sB!q8"   # â† ton mot de passe myST
#
#  Alternative : variables d'environnement dans ton shell
#  (plus propre si tu partages le repo)
#
#    export STEDGEAI_USERNAME="ton@email.com"
#    export STEDGEAI_PASSWORD="ton_mot_de_passe"
#
#  Ou dans un fichier ~/.zshrc / ~/.bash_profile pour que ce
#  soit permanent :
#    echo 'export STEDGEAI_USERNAME="ton@email.com"' >> ~/.zshrc
#    echo 'export STEDGEAI_PASSWORD="ton_mdp"'       >> ~/.zshrc
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Couleurs terminal (compatible macOS/Linux)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class C:
    BOLD   = "\033[1m"
    CYAN   = "\033[0;36m"
    GREEN  = "\033[0;32m"
    YELLOW = "\033[1;33m"
    RED    = "\033[0;31m"
    BLUE   = "\033[0;34m"
    DIM    = "\033[2m"
    RESET  = "\033[0m"


def banner(text):
    w = 62
    print(f"\n{C.BOLD}{C.CYAN}â•”{'â•'*w}â•—{C.RESET}")
    print(f"{C.BOLD}{C.CYAN}â•‘  {text:<{w-2}}â•‘{C.RESET}")
    print(f"{C.BOLD}{C.CYAN}â•š{'â•'*w}â•{C.RESET}")


def section(n, text):
    print(f"\n{C.BOLD}{C.YELLOW}â”€â”€ [{n}] {text} {'â”€'*(50-len(text))}{C.RESET}")


def ok(text):     print(f"  {C.GREEN}âœ… {text}{C.RESET}")
def warn(text):   print(f"  {C.YELLOW}âš ï¸  {text}{C.RESET}")
def err(text):    print(f"  {C.RED}âŒ {text}{C.RESET}")
def info(text):   print(f"  {C.BLUE}â„¹  {text}{C.RESET}")
def dim(text):    print(f"  {C.DIM}{text}{C.RESET}")


def table_row(label, value, width=32):
    print(f"  {C.BOLD}{label:<{width}}{C.RESET}{value}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Recherche du SDK stm32ai_dc
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SDK_CANDIDATES = [
    # Relatifs au dossier du script
    Path(__file__).parent / "stm32ai-modelzoo-services" / "common",
    Path(__file__).parent.parent / "stm32ai-modelzoo-services" / "common",
    # Dossier courant
    Path.cwd() / "stm32ai-modelzoo-services" / "common",
    Path.cwd() / "common",
    # Home
    Path.home() / "stm32ai-modelzoo-services" / "common",
    Path.home() / "Code" / "stm32ai-modelzoo-services" / "common",
    Path.home() / "code" / "stm32ai-modelzoo-services" / "common",
]

# Noms possibles du module (ST a renommÃ© en cours de route)
SDK_MODULE_NAMES = ["stm32ai_dc", "stedgeai_dc", "stm32ai"]


def find_sdk():
    """Cherche le dossier 'common/' contenant le module SDK."""
    for candidate in SDK_CANDIDATES:
        for mod in SDK_MODULE_NAMES:
            if (candidate / mod).exists() or (candidate / f"{mod}.py").exists():
                return candidate, mod
    return None, None


def import_sdk(sdk_path, module_name):
    """Importe les classes principales du SDK ST Edge AI.

    IMPORTANT : Stm32Ai et CliParameters doivent venir du mÃªme chemin d'import.
    Si Stm32Ai vient de 'common.stm32ai_dc' mais CliParameters de 'stm32ai_dc',
    le SDK lÃ¨ve une erreur de type mismatch. On force donc 'common.stm32ai_dc'
    en prioritÃ© (chemin canonique).
    """
    parent = sdk_path.parent
    sys.path.insert(0, str(sdk_path))   # pour import stm32ai_dc direct
    sys.path.insert(0, str(parent))     # pour from common.X import Y en interne

    # PrioritÃ© : common.stm32ai_dc (chemin canonique que Stm32Ai utilise en interne)
    # â†’ garantit que Stm32Ai et CliParameters partagent le mÃªme module
    import_attempts = [
        (f"common.{module_name}", ["Stm32Ai", "CloudBackend", "CliParameters"]),
        (f"common.{module_name}", ["Stm32Ai", "CloudBackend", "AnalyzeParameters"]),
        (module_name,             ["Stm32Ai", "CloudBackend", "CliParameters"]),
        (module_name,             ["Stm32Ai", "CloudBackend", "AnalyzeParameters"]),
        (module_name,             ["STM32AI", "Cloud", "Parameters"]),
    ]

    last_error = None
    for mod_name, class_names in import_attempts:
        try:
            mod = __import__(mod_name, fromlist=class_names)
            classes = {name: getattr(mod, name) for name in class_names if hasattr(mod, name)}
            if "Stm32Ai" in classes or "STM32AI" in classes:
                return classes, None
        except ImportError as e:
            last_error = e
            continue

    return None, last_error


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Helpers pour lire les rÃ©sultats (dataclass, dict, objet)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def safe_get(obj, *keys, default="â€”"):
    """Lit un champ depuis un objet (dataclass, dict, ou attribut)."""
    if obj is None:
        return default
    for k in keys:
        try:
            if isinstance(obj, dict):
                v = obj.get(k)
            else:
                v = getattr(obj, k, None)
            if v is not None:
                return v
        except Exception:
            pass
    return default


def fmt_bytes(val, default="â€”"):
    """Formate des bytes en KB/MB lisible."""
    if val == "â€”" or val is None:
        return default
    try:
        v = int(val)
        if v >= 1024 * 1024:
            return f"{v:,} bytes ({v/1024/1024:.2f} MB)"
        elif v >= 1024:
            return f"{v:,} bytes ({v/1024:.1f} KB)"
        return f"{v:,} bytes"
    except (ValueError, TypeError):
        return str(val)


def fmt_macc(val):
    """Formate les MACCs en M/K."""
    if val == "â€”" or val is None:
        return "â€”"
    try:
        v = int(val)
        if v >= 1_000_000:
            return f"{v:,}  ({v/1e6:.2f} M MACs)"
        elif v >= 1_000:
            return f"{v:,}  ({v/1e3:.1f} K MACs)"
        return f"{v:,}"
    except (ValueError, TypeError):
        return str(val)


def fmt_status_md(tried: bool, ok) -> str:
    """Emoji de statut Markdown pour les tableaux de bilan."""
    if not tried:
        return "ğŸŸ¡ Non tentÃ©"
    return "ğŸŸ¢ SuccÃ¨s" if ok else "ğŸ”´ Ã‰chec"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Historique global des benchmarks (master tracker)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TRACKER_FILENAME = "benchmark_history.md"

TRACKER_HEADER = """\
# ğŸ“Š Historique des benchmarks â€” ST Edge AI Developer Cloud

*Mis Ã  jour automatiquement par `stedgeai_benchmark.py`*

| Date | ModÃ¨le | Board | Format | Optim. | NPU | Upload | Analyze RAM | Analyze Flash | MACs | Bench. ms | FPS | Statut | Rapport |
|------|--------|-------|--------|--------|-----|--------|-------------|---------------|------|-----------|-----|--------|---------|
"""


def update_master_tracker(script_dir: Path, run_info: dict):
    """
    Ajoute une ligne dans benchmark_history.md (Ã  la racine du projet).
    CrÃ©e le fichier avec son en-tÃªte s'il n'existe pas encore.

    run_info attendu :
        date, model, board, fmt, optimization,
        npu_cell, upload_tried, upload_ok,
        a_ram, a_flash, a_macc,
        b_ms, b_fps,
        analyze_ok, bench_ok,
        report_path
    """
    tracker_path = script_dir / TRACKER_FILENAME

    upload_cell = fmt_status_md(
        run_info.get("upload_tried", False),
        run_info.get("upload_ok", False),
    )

    # Statut global
    if run_info.get("bench_ok"):
        global_status = "ğŸŸ¢ Complet"
    elif run_info.get("analyze_ok"):
        global_status = "ğŸŸ¡ Analyze seul"
    else:
        global_status = "ğŸ”´ Ã‰chec"

    # Lien relatif vers le rapport individuel
    report_path = run_info.get("report_path", "")
    try:
        rel = Path(report_path).relative_to(script_dir)
        report_link = f"[â†’]({rel})"
    except (ValueError, TypeError):
        report_link = f"[â†’]({Path(report_path).name})" if report_path else "â€”"

    row = (
        f"| {run_info.get('date','â€”')} "
        f"| `{run_info.get('model','â€”')}` "
        f"| `{run_info.get('board','â€”')}` "
        f"| `{run_info.get('fmt','â€”')}` "
        f"| {run_info.get('optimization','â€”')} "
        f"| {run_info.get('npu_cell','â€”')} "
        f"| {upload_cell} "
        f"| {run_info.get('a_ram','â€”')} "
        f"| {run_info.get('a_flash','â€”')} "
        f"| {run_info.get('a_macc','â€”')} "
        f"| {run_info.get('b_ms','â€”')} "
        f"| {run_info.get('b_fps','â€”')} "
        f"| {global_status} "
        f"| {report_link} |\n"
    )

    if not tracker_path.exists():
        tracker_path.write_text(TRACKER_HEADER + row, encoding="utf-8")
    else:
        tracker_path.write_text(
            tracker_path.read_text(encoding="utf-8") + row,
            encoding="utf-8"
        )

    return tracker_path


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DÃ©tection et configuration NPU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Patterns de boards avec NPU et leur type
# ClÃ© = sous-chaÃ®ne Ã  chercher dans le nom de board (insensible Ã  la casse)
NPU_BOARD_PATTERNS = {
    "N6":   "neural_art",      # STM32N6* â€” Neural-ART NPU (600 GOPS, 4 Compute Arrays)
    "MP2":  "hw_accelerator",  # STM32MP2* â€” MPU avec GPU/HW accelerator
    "MP13": "hw_accelerator",  # STM32MP13*
    "MP15": "hw_accelerator",  # STM32MP15*
    "MP25": "hw_accelerator",  # STM32MP25*
}

NPU_TYPE_LABELS = {
    "neural_art":     "ğŸ”® Neural-ART NPU (STM32N6) â€” 600 GOPS, 4 Compute Arrays",
    "hw_accelerator": "âš¡ HW Accelerator / GPU (STM32MP*)",
}

# Valeurs par dÃ©faut "classiques" recommandÃ©es pour Neural-ART
NEURAL_ART_CLASSIC_DEFAULTS = {
    "enable_epoch_controller":  True,   # amÃ©liore le dÃ©bit NPU (recommandÃ©)
    "cache_maintenance":        True,   # cohÃ©rence cache/NPU (recommandÃ©)
    "enable_virtual_mem_pools": True,   # utilise la RAM externe efficacement
    "Oauto":                    True,   # laisse le compilateur Neural-ART optimiser
    "Omax_ca_pipe":             0,      # 0 = auto (1-4 = nb max de Compute Arrays)
    "disable_sw_fallback":      False,  # False = fallback CPU activÃ© pour ops non supportÃ©es
    "no_hw_sw_parallelism":     False,  # False = parallÃ©lisme CPU/NPU autorisÃ©
    "max_ram_size":             None,   # None = pas de limite de RAM NPU
}


def detect_npu(board_name: str):
    """Retourne le type NPU dÃ©tectÃ© pour une board, ou None si pas de NPU."""
    board_upper = board_name.upper()
    for pattern, npu_type in NPU_BOARD_PATTERNS.items():
        if pattern.upper() in board_upper:
            return npu_type
    return None


def _ask_bool(prompt: str, default: bool) -> bool:
    """Demande oui/non avec valeur par dÃ©faut."""
    default_str = "Y/n" if default else "y/N"
    ans = input(f"    {prompt} [{default_str}] : ").strip().lower()
    if ans == "":
        return default
    return ans in ("y", "yes", "o", "oui", "1", "true")


def _ask_int(prompt: str, default, allow_none=False):
    """Demande un entier avec valeur par dÃ©faut."""
    default_str = str(default) if default is not None else "auto"
    ans = input(f"    {prompt} [{default_str}] : ").strip()
    if ans == "":
        return default
    if allow_none and ans.lower() in ("none", "auto", "-", ""):
        return None
    try:
        return int(ans)
    except ValueError:
        warn(f"Valeur invalide '{ans}', utilisation de {default}")
        return default


def _neural_art_power_user_menu(aton: dict) -> dict:
    """Menu interactif dÃ©taillÃ© pour les paramÃ¨tres AtonParameters (Neural-ART NPU)."""
    print()
    print(f"  {C.BOLD}{C.YELLOW}â”â”â” Neural-ART Power User â€” Compilateur NPU STM32N6 â”â”â”{C.RESET}")
    print()
    print(f"  {C.DIM}Les options suivantes contrÃ´lent le compilateur Neural-ART.")
    print(f"  Appuyez sur EntrÃ©e pour garder la valeur par dÃ©faut (entre crochets).{C.RESET}")
    print()

    aton = aton.copy()

    # Options boolÃ©ennes avec leurs explications
    bool_options = [
        ("enable_epoch_controller",
         "Epoch Controller (pipeline NPU â€” amÃ©liore le dÃ©bit, recommandÃ©)",
         True),
        ("cache_maintenance",
         "Cache Maintenance (cohÃ©rence mÃ©moire cache/NPU, recommandÃ©)",
         True),
        ("enable_virtual_mem_pools",
         "Virtual Memory Pools (utilise la RAM externe pour les tenseurs)",
         True),
        ("Oauto",
         "Auto-optimisation (laisse Neural-ART choisir les meilleures options)",
         True),
        ("disable_sw_fallback",
         "DÃ©sactiver fallback CPU (les ops non-NPU lÃ¨veront une erreur)",
         False),
        ("no_hw_sw_parallelism",
         "DÃ©sactiver parallÃ©lisme CPU/NPU (debug uniquement)",
         False),
    ]

    print(f"  {C.BOLD}â–¸ Options boolÃ©ennes :{C.RESET}")
    for key, label, default in bool_options:
        current = aton.get(key, default)
        aton[key] = _ask_bool(label, current)

    print()
    print(f"  {C.BOLD}â–¸ Options numÃ©riques :{C.RESET}")
    print(f"  {C.DIM}  Omax_ca_pipe : nombre max de Compute Arrays dans le pipeline")
    print(f"                 0 = automatique (recommandÃ©), 1â€“4 = valeur fixe{C.RESET}")
    aton["Omax_ca_pipe"] = _ask_int("Omax_ca_pipe (0=auto, 1-4=fixe)", aton.get("Omax_ca_pipe", 0))

    print()
    print(f"  {C.DIM}  max_ram_size : limite max de RAM allouÃ©e au NPU (en bytes)")
    print(f"                 0 ou vide = aucune limite{C.RESET}")
    aton["max_ram_size"] = _ask_int("max_ram_size en bytes (vide=illimitÃ©)", aton.get("max_ram_size"), allow_none=True)

    print()
    ok("Configuration Neural-ART power user appliquÃ©e :")
    print()
    for k, v in aton.items():
        if v is not None:
            color = C.GREEN if v is True else (C.YELLOW if v is False else C.CYAN)
            print(f"    {C.DIM}{k:<35}{C.RESET} {color}{v}{C.RESET}")
    print()

    return aton


def _hw_accelerator_power_user_menu(npu_config: dict) -> dict:
    """Menu interactif pour les paramÃ¨tres MpuParameters (HW Accelerator STM32MP*)."""
    print()
    print(f"  {C.BOLD}{C.YELLOW}â”â”â” HW Accelerator Power User â€” STM32MP* â”â”â”{C.RESET}")
    print()

    print(f"  {C.BOLD}â–¸ Moteur d'exÃ©cution :{C.RESET}")
    print(f"    1. HW_ACCELERATOR  (GPU/NPU matÃ©riel)  â† recommandÃ©")
    print(f"    2. CPU             (fallback pure CPU)")
    ans = input(f"    Choix [1] : ").strip()
    npu_config["mpu_engine"] = "CPU" if ans == "2" else "HW_ACCELERATOR"

    print()
    nb = _ask_int("Nombre de cÅ“urs CPU utilisÃ©s (1-8)", npu_config.get("mpu_nb_cores", 2))
    npu_config["mpu_nb_cores"] = max(1, min(8, nb if nb else 2))

    print()
    ok(f"Config : moteur={npu_config['mpu_engine']}, cÅ“urs={npu_config['mpu_nb_cores']}")
    return npu_config


# Formats incompatibles avec le compilateur Neural-ART NPU
NPU_INCOMPATIBLE_FORMATS = {".h5", ".keras"}
# TFLite et ONNX sont les formats supportÃ©s par le NPU
NPU_REQUIRED_FORMATS = {".tflite", ".onnx"}


def configure_npu_interactive(board: str, model_path: Path = None) -> dict | None:
    """
    DÃ©tecte le NPU de la board et propose une configuration interactive.

    Retourne un dict de config NPU, ou None si NPU non activÃ©.
    Structure du dict retournÃ© :
    {
        "npu_type": "neural_art" | "hw_accelerator",
        "enabled": True,
        # Pour neural_art :
        "stNeuralArt": "default",
        "series": "stm32n6",
        "aton": { ... }  # kwargs pour AtonParameters
        # Pour hw_accelerator :
        "mpu_engine": "HW_ACCELERATOR",
        "mpu_nb_cores": 2,
    }
    """
    npu_type = detect_npu(board)
    if npu_type is None:
        info("Pas de NPU dÃ©tectÃ© pour cette board â€” exÃ©cution sur CPU standard.")
        return None

    print()
    print(f"  {C.BOLD}{C.CYAN}NPU dÃ©tectÃ© !{C.RESET}  {NPU_TYPE_LABELS.get(npu_type, npu_type)}")
    print()

    # â”€â”€ VÃ©rification de compatibilitÃ© format/NPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if model_path is not None and npu_type == "neural_art":
        ext = model_path.suffix.lower()
        if ext in NPU_INCOMPATIBLE_FORMATS:
            warn(f"Format '{ext}' incompatible avec le Neural-ART NPU !")
            warn("Le compilateur Neural-ART n'accepte que .tflite ou .onnx.")
            print()
            print(f"  {C.DIM}Pour utiliser le NPU, convertissez d'abord le modÃ¨le :")
            print(f"  {C.CYAN}  # Keras â†’ TFLite (Python)  :{C.RESET}")
            print(f"  {C.DIM}  import tensorflow as tf")
            print(f"  {C.DIM}  converter = tf.lite.TFLiteConverter.from_keras_model(model)")
            print(f"  {C.DIM}  tflite_model = converter.convert()")
            print(f"  {C.DIM}  open('model.tflite', 'wb').write(tflite_model)")
            print()
            info("â†’ L'analyze sera lancÃ© sans NPU (CPU uniquement).")
            info("â†’ Le benchmark sera tentÃ© sans NPU.")
            return None
        elif ext not in NPU_REQUIRED_FORMATS:
            warn(f"Format '{ext}' â€” compatibilitÃ© NPU incertaine (recommandÃ© : .tflite ou .onnx).")

    if npu_type == "neural_art":
        info("Le Neural-ART NPU peut accÃ©lÃ©rer significativement l'infÃ©rence.")
        info("Config classique : epoch controller + virtual mem pools + cache maintenance.")
        info("STM32N6 Neural-ART : 600 GOPS â€” 4 Compute Arrays @ 800 MHz.")
    elif npu_type == "hw_accelerator":
        info("Le HW Accelerator (GPU) peut accÃ©lÃ©rer l'infÃ©rence sur STM32MP*.")
        info("Config classique : moteur HW_ACCELERATOR, 2 cÅ“urs.")

    print()
    ans = input("  Activer le NPU ? [Y/n] : ").strip().lower()
    if ans in ("n", "no", "non"):
        info("NPU dÃ©sactivÃ© â€” exÃ©cution sur CPU uniquement.")
        return None

    # Construire la config de base (valeurs classiques)
    npu_config = {"npu_type": npu_type, "enabled": True}

    if npu_type == "neural_art":
        npu_config.update({
            "stNeuralArt": "default",
            "series":      "stm32n6",   # lowercase attendu par le SDK
            "aton":        NEURAL_ART_CLASSIC_DEFAULTS.copy(),
        })
        print()
        ok("Config Neural-ART classique sÃ©lectionnÃ©e :")
        print()
        for k, v in npu_config["aton"].items():
            if v is not None:
                color = C.GREEN if v is True else (C.YELLOW if v is False else C.CYAN)
                print(f"    {C.DIM}{k:<35}{C.RESET} {color}{v}{C.RESET}")
        print()

        ans_pu = input("  Ouvrir le menu power user NPU pour modifier ces valeurs ? [y/N] : ").strip().lower()
        if ans_pu in ("y", "yes", "o", "oui"):
            npu_config["aton"] = _neural_art_power_user_menu(npu_config["aton"])

    elif npu_type == "hw_accelerator":
        npu_config.update({
            "mpu_engine":   "HW_ACCELERATOR",
            "mpu_nb_cores": 2,
        })
        ok("Config HW Accelerator classique : moteur=HW_ACCELERATOR, 2 cÅ“urs.")

        print()
        ans_pu = input("  Ouvrir le menu power user NPU ? [y/N] : ").strip().lower()
        if ans_pu in ("y", "yes", "o", "oui"):
            npu_config = _hw_accelerator_power_user_menu(npu_config)

    return npu_config


def _try_load_aton_cls(sdk_module_name: str):
    """Tente de charger la classe AtonParameters depuis le SDK."""
    for mod_name in [
        f"common.{sdk_module_name}",
        sdk_module_name,
        "common.stm32ai_dc",
        "common.stedgeai_dc",
        "stm32ai_dc",
        "stedgeai_dc",
    ]:
        try:
            mod = importlib.import_module(mod_name)
            if hasattr(mod, "AtonParameters"):
                return mod.AtonParameters
        except ImportError:
            continue
    return None


def _make_params(model_name: str, board: str, optimization: str,
                  extra_kwargs: dict, Params_cls) -> object:
    """Construit un objet CliParameters avec fallback de compatibilitÃ©."""
    base = {"model": model_name, "target": board, "optimization": optimization}
    base.update(extra_kwargs)
    try:
        return Params_cls(**base)
    except TypeError:
        # Certaines versions utilisent 'board' au lieu de 'target'
        try:
            fb = {k: v for k, v in base.items() if k != "target"}
            fb["board"] = board
            return Params_cls(**fb)
        except TypeError:
            return Params_cls(model=model_name)


def build_analyze_params(model_name: str, board: str, optimization: str, Params_cls):
    """
    ParamÃ¨tres SANS config NPU pour l'Ã©tape d'analyze.

    L'endpoint /analyze du cloud n'accepte pas les champs NPU (stNeuralArt,
    series, atonnOptions). On passe uniquement les params de base.
    Le fichier doit avoir Ã©tÃ© uploadÃ© au prÃ©alable (model = basename uniquement).
    """
    return _make_params(model_name, board, optimization, {}, Params_cls)


def build_benchmark_params(model_name: str, board: str, optimization: str,
                            npu_config, Params_cls, sdk_module_name="stm32ai_dc"):
    """
    ParamÃ¨tres AVEC config NPU pour l'Ã©tape de benchmark.

    L'endpoint /benchmark accepte stNeuralArt, series et atonnOptions.
    Le fichier doit avoir Ã©tÃ© uploadÃ© (model = basename uniquement).
    """
    extra = {}

    if npu_config and npu_config.get("enabled"):
        npu_type = npu_config.get("npu_type")

        if npu_type == "neural_art":
            extra["stNeuralArt"] = npu_config.get("stNeuralArt", "default")
            extra["series"]      = npu_config.get("series", "stm32n6")

            # AtonParameters â€” optionnel selon la version du SDK
            aton_kwargs = npu_config.get("aton", {})
            if aton_kwargs:
                AtonCls = _try_load_aton_cls(sdk_module_name)
                if AtonCls:
                    filtered = {k: v for k, v in aton_kwargs.items() if v is not None}
                    try:
                        extra["atonnOptions"] = AtonCls(**filtered)
                        info(f"AtonParameters appliquÃ© ({len(filtered)} options NPU)")
                    except Exception as e:
                        warn(f"AtonParameters incompatible avec cette version SDK : {e}")
                        warn("â†’ stNeuralArt='default' sera quand mÃªme transmis.")
                else:
                    warn("AtonParameters introuvable dans le SDK â€” options avancÃ©es ignorÃ©es.")

        elif npu_type == "hw_accelerator":
            info("Boards STM32MP* : l'accÃ©lÃ©rateur HW est sÃ©lectionnÃ© cÃ´tÃ© cloud.")

    return _make_params(model_name, board, optimization, extra, Params_cls)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GÃ©nÃ©ration du rapport Markdown
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_markdown(model_path, board, optimization, analyze_result, bench_result=None, logs="", npu_config=None, run_status=None):
    now        = datetime.now().strftime("%Y-%m-%d %H:%M")
    model_name = Path(model_path).name
    p          = Path(model_path)
    model_size_kb = p.stat().st_size / 1024 if p.exists() else 0.0

    # Champs analyze
    a_ram    = safe_get(analyze_result, "ram_size",   "estimated_ram_size",   "ram")
    a_flash  = safe_get(analyze_result, "flash_size", "estimated_flash_size", "flash")
    a_macc   = safe_get(analyze_result, "macc",       "total_macc",           "macs")
    a_params = safe_get(analyze_result, "weights",    "params_count",         "parameters")
    a_layers = safe_get(analyze_result, "num_layers", "layers_count")

    # Champs benchmark
    b_ram      = safe_get(bench_result, "ram_size",      "ram")
    b_flash    = safe_get(bench_result, "flash_size",    "flash")
    b_infer_ms = safe_get(bench_result, "inference_time","latency_ms", "duration_ms")
    b_cpu_mhz  = safe_get(bench_result, "cpu_freq_mhz",  "cpu_frequency", "clock_mhz")
    b_cycles   = safe_get(bench_result, "cycles",        "clock_cycles")

    b_fps = "â€”"
    try:
        if b_infer_ms not in ("â€”", None) and float(b_infer_ms) > 0:
            b_fps = f"{1000 / float(b_infer_ms):.1f}"
    except (ValueError, TypeError):
        pass

    # Section NPU dans les tags YAML
    npu_tag = ""
    if npu_config and npu_config.get("enabled"):
        npu_tag = f'\nnpu: "{npu_config.get("npu_type", "unknown")}"'

    # Table de rÃ©fÃ©rence hardware
    hw_table = """| Carte | RAM | Flash | Famille |
|-------|-----|-------|---------|
| NUCLEO-H7A3ZI-Q | 1 400 KB | 2 MB | STM32H7 |
| B-L475E-IOT01A | 128 KB | 1 MB | STM32L4 |
| NUCLEO-L4R5ZI | 640 KB | 2 MB | STM32L4+ |
| NUCLEO-F401RE | 96 KB | 512 KB | STM32F4 |
| STM32N6570-DK | 4 200 KB | ext. Flash | STM32N6 (NPU) |
| NUCLEO-U5A5ZJ-Q | 2 500 KB | 4 MB | STM32U5 |"""

    # â”€â”€ Bandeau de statut â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    rs = run_status or {}

    def _s(key):
        """Retourne l'emoji de statut pour une Ã©tape donnÃ©e."""
        step = rs.get(key, {})
        return fmt_status_md(step.get("tried", False), step.get("ok", False))

    def _d(key):
        """Retourne le dÃ©tail textuel d'une Ã©tape."""
        return rs.get(key, {}).get("detail", "â€”")

    # Statut global du run
    if rs.get("benchmark", {}).get("ok"):
        run_global = "ğŸŸ¢ Complet â€” analyze + benchmark rÃ©ussis"
    elif rs.get("analyze", {}).get("ok"):
        run_global = "ğŸŸ¡ Partiel â€” analyze rÃ©ussi, benchmark non effectuÃ© ou en Ã©chec"
    else:
        run_global = "ğŸ”´ Ã‰chec â€” analyze non concluant"

    # RÃ©sumÃ© RAM/Flash compact pour le bandeau
    a_ram_short  = fmt_bytes(a_ram)  if a_ram  not in ("â€”", None) else "â€”"
    a_flash_short= fmt_bytes(a_flash)if a_flash not in ("â€”", None) else "â€”"
    b_ms_val     = safe_get(bench_result, "duration_ms") if bench_result else "â€”"

    status_section = f"""\
## ğŸ“‹ Statut du run

> {run_global}

| Ã‰tape | Statut | RÃ©sumÃ© |
|-------|--------|--------|
| â˜ï¸ Upload | {_s("upload")} | {_d("upload")} |
| ğŸ§  Analyze | {_s("analyze")} | {_d("analyze")} â€” [voir rÃ©sultats](#-analyse-statique-analyze) |
| âš¡ Benchmark | {_s("benchmark")} | {_d("benchmark")} â€” [voir rÃ©sultats](#-benchmark-sur-matÃ©riel-rÃ©el) |
| ğŸ”® NPU | {_s("npu")} | {_d("npu")} |

"""
    # â”€â”€ /Bandeau de statut â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    md = f"""---
title: "Benchmark ST Edge AI â€” {model_name}"
date: "{now}"
model: "{model_name}"
board: "{board}"
optimization: "{optimization}"{npu_tag}
tags: [stedgeai, benchmark, edge-ai, stm32]
---

# ğŸ“Š Rapport ST Edge AI Developer Cloud

> **ModÃ¨le :** `{model_name}`  ({model_size_kb:.1f} KB sur disque)
> **Board :** `{board}`
> **Optimisation :** `{optimization}`
> **Date :** {now}

---

{status_section}---

## ğŸ§  Analyse statique (Analyze)

*Estimation des ressources nÃ©cessaires sans exÃ©cution rÃ©elle.*

| MÃ©trique | Valeur brute | Lisible |
|----------|-------------|---------|
| RAM estimÃ©e | `{a_ram}` | {fmt_bytes(a_ram)} |
| Flash estimÃ©e | `{a_flash}` | {fmt_bytes(a_flash)} |
| MACCs totales | `{a_macc}` | {fmt_macc(a_macc)} |
| ParamÃ¨tres (poids) | `{a_params}` | â€” |
| Nombre de couches | `{a_layers}` | â€” |

"""

    if bench_result is not None:
        md += f"""---

## âš¡ Benchmark sur matÃ©riel rÃ©el

*Mesures effectuÃ©es sur la board `{board}` hÃ©bergÃ©e dans la board farm ST.*

| MÃ©trique | Valeur brute | Lisible |
|----------|-------------|---------|
| RAM rÃ©elle | `{b_ram}` | {fmt_bytes(b_ram)} |
| Flash rÃ©elle | `{b_flash}` | {fmt_bytes(b_flash)} |
| Temps d'infÃ©rence | `{b_infer_ms}` ms | â€” |
| FrÃ©quence CPU | `{b_cpu_mhz}` MHz | â€” |
| Cycles | `{b_cycles}` | â€” |
| **DÃ©bit estimÃ©** | **{b_fps} inf/sec** | â€” |

"""
    else:
        md += """---

## âš¡ Benchmark sur matÃ©riel rÃ©el

*Non exÃ©cutÃ© (lancÃ© avec `--no-benchmark` ou refusÃ© interactivement).*

"""

    # Section NPU
    if npu_config and npu_config.get("enabled"):
        npu_type = npu_config.get("npu_type")
        npu_label = NPU_TYPE_LABELS.get(npu_type, npu_type)

        md += f"""---

## ğŸ”® Configuration NPU

**Type :** {npu_label}

"""
        if npu_type == "neural_art":
            st_na  = npu_config.get("stNeuralArt", "default")
            series = npu_config.get("series", "STM32N6")
            md += f"**stNeuralArt :** `{st_na}`  \n**series :** `{series}`\n\n"
            aton = npu_config.get("aton", {})
            if aton:
                md += "**AtonParameters (Neural-ART compiler) :**\n\n"
                md += "| ParamÃ¨tre | Valeur |\n|-----------|--------|\n"
                for k, v in aton.items():
                    if v is not None:
                        icon = "âœ…" if v is True else ("â¬œ" if v is False else "ğŸ”¢")
                        md += f"| `{k}` | {icon} `{v}` |\n"
                md += "\n"
        elif npu_type == "hw_accelerator":
            engine   = npu_config.get("mpu_engine", "HW_ACCELERATOR")
            nb_cores = npu_config.get("mpu_nb_cores", 2)
            md += f"**Moteur :** `{engine}`  \n**CÅ“urs :** `{nb_cores}`\n\n"
    else:
        md += """---

## ğŸ”® Configuration NPU

*NPU non utilisÃ© pour ce benchmark â€” exÃ©cution sur CPU standard.*

"""

    md += f"""---

## ğŸ“Œ InterprÃ©tation rapide

### CompatibilitÃ© mÃ©moire estimÃ©e

| Cible | RAM dispo | Flash dispo | Compatible ? |
|-------|-----------|-------------|-------------|
| STM32H7 (NUCLEO-H7A3ZI-Q) | 1 400 KB | 2 048 KB | {"âœ…" if a_ram not in ("â€”", None) and isinstance(a_ram, (int,str)) else "?"} |
| STM32L4 (B-L475E-IOT01A)  | 128 KB   | 1 024 KB | â€” |
| STM32F4 (NUCLEO-F401RE)   | 96 KB    | 512 KB   | â€” |
| STM32U5 (NUCLEO-U5A5ZJ-Q) | 2 560 KB | 4 096 KB | â€” |
| STM32N6 (NPU)             | 4 200 KB | ext.     | â€” |

> âš ï¸ Mettre Ã  jour manuellement la colonne "Compatible ?" avec les valeurs RAM/Flash ci-dessus.

### Boards de rÃ©fÃ©rence ST

{hw_table}

"""

    if logs:
        md += f"""---

## ğŸ“‹ Logs / Erreurs

```
{logs}
```

"""

    md += f"""---

## ğŸ”— Ressources

- [ST Edge AI Developer Cloud](https://stedgeai-dc.st.com)
- [stm32ai-modelzoo-services](https://github.com/STMicroelectronics/stm32ai-modelzoo-services)
- [Getting Started Wiki](https://wiki.st.com/stm32mcu/wiki/AI:Getting_started_with_ST_Edge_AI_Developer_Cloud)
- [Neural-ART NPU â€” STM32N6](https://wiki.st.com/stm32mcu/wiki/AI:Neural-ART_NPU)

---

*GÃ©nÃ©rÃ© automatiquement par `stedgeai_benchmark.py` le {now}*
"""
    return md


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Scan des modÃ¨les disponibles
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SUPPORTED_EXT = {".tflite", ".h5", ".keras", ".onnx", ".pb", ".pt", ".pth"}


MIN_MODEL_SIZE_KB = 5   # Les stubs git-lfs font ~0.1 KB, les vrais modÃ¨les > 5 KB
MAX_DEPTH         = 5   # Profondeur max de recherche depuis le dossier racine


def scan_models(root: Path, max_depth: int = MAX_DEPTH) -> list:
    """Scanne rÃ©cursivement root (jusqu'Ã  max_depth) et retourne les vrais fichiers modÃ¨les.

    Filtre les stubs git-lfs (< MIN_MODEL_SIZE_KB).
    """
    seen   = set()
    result = []
    root_depth = len(root.parts)

    for ext in SUPPORTED_EXT:
        for p in sorted(root.rglob(f"*{ext}"), key=lambda x: str(x)):
            if p in seen:
                continue
            # Limite de profondeur
            if len(p.parts) - root_depth > max_depth:
                continue
            seen.add(p)
            try:
                if p.stat().st_size >= MIN_MODEL_SIZE_KB * 1024:
                    result.append(p)
            except OSError:
                pass
    return result


def pick_model_interactive(source_models_dir: Path) -> Path:
    """Menu interactif en 2 Ã©tapes : 1) repo  2) fichier modÃ¨le."""

    # â”€â”€ Ã‰tape A : lister les repos (sous-dossiers de niveau 2) â”€â”€â”€
    # Structure : Source_Models/Framework/Repo/
    repos = []
    for framework_dir in sorted(source_models_dir.iterdir()):
        if not framework_dir.is_dir() or framework_dir.name.startswith("."):
            continue
        for repo_dir in sorted(framework_dir.iterdir()):
            if repo_dir.is_dir():
                repos.append(repo_dir)
        # Si le framework n'a pas de sous-repos, l'ajouter directement
        if not any(d.is_dir() for d in framework_dir.iterdir() if not d.name.startswith(".")):
            repos.append(framework_dir)

    if not repos:
        return None

    # Compter les modÃ¨les par repo pour affichage
    print(f"\n  {C.BOLD}Repos disponibles :{C.RESET}\n")
    repo_model_counts = {}
    for repo in repos:
        models = scan_models(repo, max_depth=4)
        repo_model_counts[repo] = models

    current_fw = None
    repo_idx = {}
    display_i = 1
    for repo in repos:
        fw = repo.parent.name
        if fw != current_fw:
            current_fw = fw
            fw_label = {"PyTorch": "ğŸŸ  PyTorch", "TensorFlow_Keras": "ğŸ”µ TensorFlow/Keras", "ONNX": "ğŸŸ£ ONNX"}.get(fw, f"ğŸ“ {fw}")
            print(f"\n  {C.BOLD}{fw_label}{C.RESET}")
        count = len(repo_model_counts[repo])
        count_str = f"{C.GREEN}{count} modÃ¨le(s){C.RESET}" if count > 0 else f"{C.DIM}aucun modÃ¨le{C.RESET}"
        print(f"  {C.BOLD}{display_i:>3}.{C.RESET}  {repo.name:<50} {count_str}")
        repo_idx[display_i] = repo
        display_i += 1

    print()
    choice_repo = input("  NumÃ©ro du repo : ").strip()
    if not choice_repo.isdigit() or int(choice_repo) not in repo_idx:
        return None
    selected_repo = repo_idx[int(choice_repo)]

    # â”€â”€ Ã‰tape B : lister les modÃ¨les dans ce repo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    models = repo_model_counts[selected_repo]
    if not models:
        return None

    if len(models) == 1:
        ok(f"Un seul modÃ¨le trouvÃ© â†’ sÃ©lection automatique : {models[0].name}")
        return models[0]

    print(f"\n  {C.BOLD}ModÃ¨les dans {selected_repo.name}/ :{C.RESET}\n")
    for i, p in enumerate(models, 1):
        size_kb = p.stat().st_size / 1024
        try:
            rel = p.relative_to(selected_repo)
        except ValueError:
            rel = p.name
        print(f"  {C.BOLD}{i:>3}.{C.RESET}  {str(rel):<60} {C.DIM}{size_kb:>8.1f} KB{C.RESET}")

    print()
    choice_model = input("  NumÃ©ro du modÃ¨le : ").strip()
    if not choice_model.isdigit() or not (1 <= int(choice_model) <= len(models)):
        return None
    return models[int(choice_model) - 1]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description="Benchmark interactif via ST Edge AI Developer Cloud",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument("--model",        "-m", help="Chemin du modÃ¨le (.tflite / .h5 / .onnx)")
    parser.add_argument("--board",        "-b", help="Nom de la board STM32 cible")
    parser.add_argument("--optimization", "-o",
                        choices=["balanced", "time", "ram"],
                        default="balanced",
                        help="StratÃ©gie d'optimisation (dÃ©faut: balanced)")
    parser.add_argument("--no-benchmark", action="store_true",
                        help="Analyse statique uniquement, sans benchmark hardware")
    parser.add_argument("--output",       "-O",
                        help="Chemin du fichier Markdown de sortie (auto-gÃ©nÃ©rÃ© si absent)")
    args = parser.parse_args()

    banner("ST Edge AI Developer Cloud â€” Benchmark interactif")
    print(f"  {C.DIM}Smart Thermostat Edge AI â€” {datetime.now().strftime('%Y-%m-%d %H:%M')}{C.RESET}")

    # â”€â”€ [1] Localisation du SDK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    section(1, "Localisation du SDK stm32ai_dc")

    sdk_path, module_name = find_sdk()

    if sdk_path:
        ok(f"SDK trouvÃ© : {sdk_path}")
        info(f"Module : {module_name}")
    else:
        err("SDK stm32ai_dc introuvable dans les emplacements standard.")
        print(f"""
  {C.BOLD}Pour installer le SDK (une seule fois) :{C.RESET}

  {C.CYAN}# Depuis le dossier "Smart Thermostat/"{C.RESET}
  git clone https://github.com/STMicroelectronics/stm32ai-modelzoo-services.git
  pip install -r stm32ai-modelzoo-services/requirements.txt

  {C.CYAN}# Puis relancer :{C.RESET}
  python stedgeai_benchmark.py --model mon_modele.tflite
""")
        sys.exit(1)

    # â”€â”€ [2] Import du SDK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    section(2, "Import du SDK")

    sdk_classes, import_err = import_sdk(sdk_path, module_name)

    if not sdk_classes:
        err(f"Import Ã©chouÃ© : {import_err}")
        info("VÃ©rifiez les dÃ©pendances :")
        info("  pip install -r stm32ai-modelzoo-services/requirements.txt")
        sys.exit(1)

    Stm32Ai_cls   = sdk_classes.get("Stm32Ai") or sdk_classes.get("STM32AI")
    Backend_cls   = sdk_classes.get("CloudBackend") or sdk_classes.get("Cloud")
    Params_cls    = sdk_classes.get("CliParameters") or sdk_classes.get("AnalyzeParameters") or sdk_classes.get("Parameters")

    ok(f"SDK importÃ© : {', '.join(sdk_classes.keys())}")

    # â”€â”€ [3] Authentification myST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    section(3, "Authentification myST")
    info("Compte sur https://my.st.com requis (gratuit)")

    username = (STEDGEAI_USERNAME.strip() or           # 1. hardcodÃ© en haut du fichier
                os.environ.get("STEDGEAI_USERNAME") or  # 2. variable d'env
                os.environ.get("STM32AI_USERNAME")  or  # 3. ancien nom de variable
                input("  Email myST : ").strip())       # 4. saisie interactive

    password = (STEDGEAI_PASSWORD.strip() or
                os.environ.get("STEDGEAI_PASSWORD") or
                os.environ.get("STM32AI_PASSWORD")  or
                getpass.getpass("  Mot de passe myST : "))

    print("  Connexion en cours...")

    ai = None
    try:
        backend = Backend_cls(username=username, password=password)
        ai = Stm32Ai_cls(backend)
        ok("ConnectÃ© au ST Edge AI Developer Cloud")
    except Exception as e:
        err(f"Connexion Ã©chouÃ©e : {e}")
        info("â†’ VÃ©rifiez vos identifiants sur https://stedgeai-dc.st.com")
        info("â†’ Si votre mot de passe contient des caractÃ¨res spÃ©ciaux (@, :, etc.),")
        info("  utilisez les variables d'env STEDGEAI_USERNAME / STEDGEAI_PASSWORD")
        sys.exit(1)

    # â”€â”€ [4] SÃ©lection du modÃ¨le (scan arborescence) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    section(4, "SÃ©lection du modÃ¨le")

    model_path = None

    if args.model:
        # Chemin fourni en argument : usage direct
        model_path = Path(args.model).expanduser().resolve()
        if not model_path.exists():
            err(f"Fichier introuvable : {model_path}")
            sys.exit(1)
    else:
        # Chercher le dossier Source_Models/
        script_dir = Path(__file__).parent.resolve()
        source_models_dir = None
        for candidate in [
            script_dir / "Source_Models",
            Path.cwd() / "Source_Models",
        ]:
            if candidate.exists() and candidate.is_dir():
                source_models_dir = candidate
                break

        if source_models_dir:
            info(f"Scan de {source_models_dir}")
            model_path = pick_model_interactive(source_models_dir)

        if model_path is None:
            # Fallback : saisie manuelle
            warn("Aucun modÃ¨le sÃ©lectionnÃ© via le menu.")
            model_path_raw = input("  Entrez le chemin complet du modÃ¨le : ").strip()
            if not model_path_raw:
                sys.exit(0)
            model_path = Path(model_path_raw).expanduser().resolve()

    if not model_path.exists():
        err(f"Fichier introuvable : {model_path}")
        sys.exit(1)

    model_size_kb = model_path.stat().st_size / 1024
    ok(f"{model_path.name}  ({model_size_kb:.1f} KB)")
    info(str(model_path))

    if model_path.suffix.lower() not in SUPPORTED_EXT:
        warn(f"Extension '{model_path.suffix}' non standard.")
        warn("Le modÃ¨le devra peut-Ãªtre Ãªtre converti en TFLite ou ONNX d'abord.")
        ans = input("  Continuer quand mÃªme ? [y/n] : ").strip().lower()
        if ans not in ("y", "yes", "o", "oui"):
            sys.exit(0)

    # â”€â”€ [5] SÃ©lection de la board â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    section(5, "SÃ©lection de la board STM32")

    board = args.board
    board_list = []

    if not board:
        print("  RÃ©cupÃ©ration des boards disponibles depuis le cloud...")
        try:
            raw_boards = ai.get_benchmark_boards()
            # Normaliser en liste de strings
            if isinstance(raw_boards, (list, tuple)):
                board_list = [
                    b if isinstance(b, str) else getattr(b, "name", str(b))
                    for b in raw_boards
                ]
            elif hasattr(raw_boards, "__iter__"):
                board_list = [
                    b if isinstance(b, str) else getattr(b, "name", str(b))
                    for b in raw_boards
                ]
            ok(f"{len(board_list)} boards disponibles")
        except Exception as e:
            warn(f"Impossible de rÃ©cupÃ©rer la liste en ligne : {e}")
            warn("Utilisation de la liste statique.")

        # Fallback : liste statique connue
        if not board_list:
            board_list = [
                "B-L475E-IOT01A",
                "NUCLEO-H7A3ZI-Q",
                "NUCLEO-L4R5ZI",
                "NUCLEO-F401RE",
                "STM32N6570-DK",
                "NUCLEO-U5A5ZJ-Q",
                "B-U585I-IOT02A",
                "STM32MP257F-EV1",
            ]

        print(f"\n  {'#':<5} {'Board'}")
        print(f"  {'â”€'*5} {'â”€'*40}")
        for i, b in enumerate(board_list, 1):
            npu_marker = ""
            if detect_npu(b) == "neural_art":
                npu_marker = f" {C.CYAN}ğŸ”® NPU Neural-ART{C.RESET}"
            elif detect_npu(b) == "hw_accelerator":
                npu_marker = f" {C.YELLOW}âš¡ HW Accelerator{C.RESET}"
            elif "H7" in b or "U5" in b:
                npu_marker = " â­"
            print(f"  {i:<5} {b}{npu_marker}")

        print()
        choice = input("  Entrez le numÃ©ro ou le nom exact de la board : ").strip()

        if choice.isdigit():
            idx = int(choice) - 1
            if 0 <= idx < len(board_list):
                board = board_list[idx]
            else:
                err(f"NumÃ©ro invalide : {choice}")
                sys.exit(1)
        else:
            board = choice

    ok(f"Board : {board}")

    # â”€â”€ [6] Configuration NPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    section(6, "Configuration NPU")

    npu_config = configure_npu_interactive(board, model_path)

    if npu_config and npu_config.get("enabled"):
        ok(f"NPU activÃ© : {NPU_TYPE_LABELS.get(npu_config.get('npu_type'), 'inconnu')}")
    else:
        info("ExÃ©cution CPU standard (sans accÃ©lÃ©ration NPU).")

    # â”€â”€ [7] Options d'optimisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    section(7, "Options d'optimisation")

    optimization = args.optimization
    print(f"  StratÃ©gie : {C.BOLD}{optimization}{C.RESET}  (balanced | time | ram)")
    if optimization == "balanced":
        dim("  â†’ Compromis Ã©quilibrÃ© RAM/Flash/vitesse (recommandÃ©)")
    elif optimization == "time":
        dim("  â†’ PrioritÃ© sur la vitesse d'infÃ©rence (plus de RAM)")
    else:
        dim("  â†’ PrioritÃ© sur la rÃ©duction RAM (peut Ãªtre plus lent)")

    run_benchmark = not args.no_benchmark
    if run_benchmark:
        print()
        ans = input("  Lancer le benchmark sur hardware rÃ©el ? (2-5 min) [y/n] : ").strip().lower()
        run_benchmark = ans in ("y", "yes", "o", "oui")

    # Initialisation du suivi de statut (alimentÃ© au fil des Ã©tapes)
    run_status = {
        "upload":    {"tried": False, "ok": False, "detail": "â€”"},
        "analyze":   {"tried": False, "ok": False, "detail": "â€”"},
        "benchmark": {"tried": False, "ok": False, "detail": "â€”"},
        "npu":       {"tried": False, "ok": False, "detail": "â€”"},
    }

    # Statut NPU (dÃ©jÃ  dÃ©terminÃ© Ã  l'Ã©tape [6])
    if npu_config and npu_config.get("enabled"):
        npu_label = NPU_TYPE_LABELS.get(npu_config.get("npu_type"), "NPU")
        run_status["npu"] = {"tried": True, "ok": True, "detail": npu_label}
    elif detect_npu(board):
        ext = model_path.suffix.lower()
        if ext in NPU_INCOMPATIBLE_FORMATS:
            run_status["npu"] = {"tried": False, "ok": False,
                                  "detail": f"Format `{ext}` incompatible â€” convertir en .tflite ou .onnx"}
        else:
            run_status["npu"] = {"tried": False, "ok": False, "detail": "DÃ©sactivÃ© par l'utilisateur"}
    else:
        run_status["npu"] = {"tried": False, "ok": False, "detail": "Pas de NPU sur cette board"}

    # â”€â”€ [8] Upload du modÃ¨le sur le cloud â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    section(8, "Upload du modÃ¨le sur le cloud")

    uploaded_model_name = model_path.name
    upload_logs = ""

    print(f"  ModÃ¨le : {uploaded_model_name}  ({model_size_kb:.1f} KB)")
    print("  Upload vers ST Edge AI Developer Cloud...")

    run_status["upload"]["tried"] = True
    try:
        cloud_models = [m.get("name", m) if isinstance(m, dict) else str(m)
                        for m in (ai.list_models() or [])]
        if uploaded_model_name in cloud_models:
            ok(f"ModÃ¨le dÃ©jÃ  prÃ©sent sur le cloud : {uploaded_model_name}")
            run_status["upload"]["ok"]     = True
            run_status["upload"]["detail"] = f"`{uploaded_model_name}` ({model_size_kb:.1f} KB) â€” dÃ©jÃ  prÃ©sent"
        else:
            uploaded = ai.upload_model(str(model_path))
            if uploaded:
                ok(f"ModÃ¨le uploadÃ© : {uploaded_model_name}")
                run_status["upload"]["ok"]     = True
                run_status["upload"]["detail"] = f"`{uploaded_model_name}` ({model_size_kb:.1f} KB)"
            else:
                warn("Upload a Ã©chouÃ© ou modÃ¨le dÃ©jÃ  prÃ©sent â€” on continue.")
                run_status["upload"]["detail"] = "RÃ©ponse ambiguÃ« du serveur"
    except Exception as e:
        warn(f"VÃ©rification cloud Ã©chouÃ©e : {e}")
        warn("â†’ Tentative d'upload quand mÃªme...")
        try:
            ai.upload_model(str(model_path))
            ok("Upload rÃ©ussi.")
            run_status["upload"]["ok"]     = True
            run_status["upload"]["detail"] = f"`{uploaded_model_name}` ({model_size_kb:.1f} KB)"
        except Exception as e2:
            err(f"Upload Ã©chouÃ© : {e2}")
            upload_logs = traceback.format_exc()
            run_status["upload"]["detail"] = f"Erreur : {e2}"

    # â”€â”€ [9] Analyze (analyse statique) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    section(9, "Analyse statique (analyze)")
    print("  Envoi de la requÃªte analyze au cloud...")
    print(f"  {C.DIM}(sans params NPU â€” l'endpoint /analyze ne les supporte pas){C.RESET}")

    analyze_result = None
    analyze_logs   = ""
    run_status["analyze"]["tried"] = True

    try:
        params = build_analyze_params(uploaded_model_name, board, optimization, Params_cls)

        # analyze(options) â€” 1 seul argument (+ self).
        # Ne pas catcher TypeError : le SDK interne en lÃ¨ve une quand le serveur
        # retourne None (bug cloud.py:101 : 'message' in None). On laisse remonter.
        analyze_result = ai.analyze(params)

        if analyze_result is None:
            raise RuntimeError("Le serveur a retournÃ© une rÃ©ponse vide. "
                               "VÃ©rifiez les logs du terminal pour l'erreur serveur.")

        ok("Analyse terminÃ©e !")
        a_ram_v   = safe_get(analyze_result, "ram_size",   "estimated_ram_size", "ram")
        a_flash_v = safe_get(analyze_result, "flash_size", "estimated_flash_size","flash")
        a_macc_v  = safe_get(analyze_result, "macc",       "total_macc",          "macs")
        run_status["analyze"]["ok"]     = True
        run_status["analyze"]["detail"] = (
            f"RAM {fmt_bytes(a_ram_v)} Â· Flash {fmt_bytes(a_flash_v)} Â· {fmt_macc(a_macc_v)}"
        )

        # Affichage des rÃ©sultats
        print()
        fields = [
            ("RAM estimÃ©e",       ["ram_size",   "estimated_ram_size",   "ram"]),
            ("Flash estimÃ©e",     ["flash_size", "estimated_flash_size", "flash"]),
            ("MACCs",             ["macc",       "total_macc",           "macs"]),
            ("ParamÃ¨tres",        ["weights",    "params_count",         "parameters"]),
            ("Nombre de couches", ["num_layers", "layers_count",         "n_layers"]),
        ]
        for label, keys in fields:
            val = safe_get(analyze_result, *keys)
            if label in ("RAM estimÃ©e", "Flash estimÃ©e"):
                table_row(label, fmt_bytes(val))
            elif label == "MACCs":
                table_row(label, fmt_macc(val))
            else:
                table_row(label, str(val))

    except Exception as e:
        err(f"Analyse Ã©chouÃ©e : {e}")
        analyze_logs = traceback.format_exc()
        run_status["analyze"]["detail"] = f"Erreur : {str(e)[:120]}"
        warn("Le rapport Markdown contiendra le log d'erreur complet.")

    # â”€â”€ [10] Benchmark (optionnel) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    bench_result = None
    bench_logs   = ""

    if run_benchmark:
        section(10, f"Benchmark sur {board}")
        print("  Envoi vers la board farm... (peut prendre 2â€“5 minutes)")
        print("  Chaque board est physiquement disponible dans les locaux ST.")
        if npu_config and npu_config.get("enabled"):
            npu_lbl = NPU_TYPE_LABELS.get(npu_config.get("npu_type"), "NPU")
            print(f"  {C.CYAN}Config NPU : {npu_lbl}{C.RESET}")

        run_status["benchmark"]["tried"] = True
        try:
            params = build_benchmark_params(
                uploaded_model_name, board, optimization,
                npu_config, Params_cls, module_name
            )

            bench_result = ai.benchmark(params, board)

            if bench_result is None:
                raise RuntimeError("Le serveur a retournÃ© une rÃ©ponse vide pour le benchmark.")

            ok(f"Benchmark terminÃ© sur {board} !")
            b_ms_v = safe_get(bench_result, "duration_ms", "inference_time", "latency_ms")
            b_fps_v = "â€”"
            try:
                if b_ms_v not in ("â€”", None) and float(b_ms_v) > 0:
                    b_fps_v = f"{1000/float(b_ms_v):.1f} inf/s"
            except (ValueError, TypeError):
                pass
            run_status["benchmark"]["ok"]     = True
            run_status["benchmark"]["detail"] = (
                f"{b_ms_v} ms Â· {b_fps_v}" if b_ms_v not in ("â€”", None) else "OK"
            )

            # Affichage
            print()
            b_fields = [
                ("RAM rÃ©elle",          ["ram_size",      "ram"]),
                ("Flash rÃ©elle",        ["flash_size",    "flash"]),
                ("Temps d'infÃ©rence",   ["inference_time","latency_ms","duration_ms"]),
                ("FrÃ©quence CPU (MHz)", ["cpu_freq_mhz",  "cpu_frequency","clock_mhz"]),
                ("Cycles",              ["cycles",        "clock_cycles"]),
            ]
            for label, keys in b_fields:
                val = safe_get(bench_result, *keys)
                if label in ("RAM rÃ©elle", "Flash rÃ©elle"):
                    table_row(label, fmt_bytes(val))
                elif label == "Temps d'infÃ©rence" and val not in ("â€”", None):
                    try:
                        fps = f"{1000 / float(val):.1f} inf/sec"
                        table_row(label, f"{val} ms  â†’  {fps}")
                    except (ValueError, TypeError):
                        table_row(label, str(val))
                else:
                    table_row(label, str(val))

        except Exception as e:
            err(f"Benchmark Ã©chouÃ© : {e}")
            bench_logs = traceback.format_exc()
            run_status["benchmark"]["detail"] = f"Erreur : {str(e)[:120]}"
            warn("Le rapport Markdown contiendra le log d'erreur complet.")
    else:
        run_status["benchmark"] = {"tried": False, "ok": False, "detail": "Non demandÃ©"}

    # â”€â”€ [11] GÃ©nÃ©ration du rapport Markdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    step_n = 11 if run_benchmark else 10
    section(step_n, "GÃ©nÃ©ration du rapport Markdown")

    all_logs = "\n\n".join(filter(None, [upload_logs, analyze_logs, bench_logs]))

    md_content = build_markdown(
        model_path     = str(model_path),
        board          = board,
        optimization   = optimization,
        analyze_result = analyze_result,
        bench_result   = bench_result if run_benchmark else None,
        logs           = all_logs,
        npu_config     = npu_config,
        run_status     = run_status,
    )

    # Fichier de sortie
    if args.output:
        out_path = Path(args.output).expanduser().resolve()
    else:
        ts       = datetime.now().strftime("%Y%m%d_%H%M")
        out_name = f"benchmark_{model_path.stem}_{board}_{ts}.md"
        out_path = model_path.parent / out_name

    out_path.write_text(md_content, encoding="utf-8")
    ok(f"Rapport sauvegardÃ© : {out_path}")

    # â”€â”€ [12] Mise Ã  jour du master tracker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    section(step_n + 1, "Mise Ã  jour de l'historique global")

    script_dir = Path(__file__).parent.resolve()
    a_ram_r  = safe_get(analyze_result, "ram_size",   "estimated_ram_size",  "ram")
    a_fla_r  = safe_get(analyze_result, "flash_size", "estimated_flash_size","flash")
    a_mac_r  = safe_get(analyze_result, "macc",       "total_macc",          "macs")
    b_ms_r   = safe_get(bench_result,   "duration_ms","inference_time",      "latency_ms")
    b_fps_r  = "â€”"
    try:
        if b_ms_r not in ("â€”", None) and float(b_ms_r) > 0:
            b_fps_r = f"{1000/float(b_ms_r):.1f}"
    except (ValueError, TypeError):
        pass

    # Cellule NPU pour le tracker
    if npu_config and npu_config.get("enabled"):
        npu_cell = f"ğŸ”® {npu_config.get('npu_type','?')}"
    elif detect_npu(board):
        npu_cell = "â¬œ NPU off"
    else:
        npu_cell = "â€”"

    tracker_path = update_master_tracker(script_dir, {
        "date":         datetime.now().strftime("%Y-%m-%d %H:%M"),
        "model":        model_path.name,
        "board":        board,
        "fmt":          model_path.suffix.lower(),
        "optimization": optimization,
        "npu_cell":     npu_cell,
        "upload_tried": run_status["upload"]["tried"],
        "upload_ok":    run_status["upload"]["ok"],
        "a_ram":        fmt_bytes(a_ram_r)  if a_ram_r  not in ("â€”", None) else "â€”",
        "a_flash":      fmt_bytes(a_fla_r)  if a_fla_r  not in ("â€”", None) else "â€”",
        "a_macc":       fmt_macc(a_mac_r),
        "b_ms":         f"{b_ms_r} ms"      if b_ms_r   not in ("â€”", None) else "â€”",
        "b_fps":        b_fps_r,
        "analyze_ok":   run_status["analyze"]["ok"],
        "bench_ok":     run_status["benchmark"]["ok"],
        "report_path":  str(out_path),
    })
    ok(f"Historique mis Ã  jour : {tracker_path}")

    # â”€â”€ Fin â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    banner("âœ… TerminÃ© !")
    print(f"  Rapport    : {C.CYAN}{out_path}{C.RESET}")
    print(f"  Historique : {C.CYAN}{tracker_path}{C.RESET}")
    print()


if __name__ == "__main__":
    main()
