---
title: "Innovative machine learning approaches for indoor air temperature forecasting in smart infrastructure"
authors:
  - "Author, First"
  - "Author, Second"
year: 2025
venue: "Scientific Reports"
publisher: "Nature / Springer Nature"
doi: "10.1038/s41598-024-85026-3"
url: "https://www.nature.com/articles/s41598-024-85026-3"
pdf_url: "https://www.nature.com/articles/s41598-024-85026-3"
tags:
  - lstm
  - indoor-temperature
  - forecasting
  - edge-ai
  - tinyml
  - microcontroller
  - embedded
  - lightweight
methods:
  - "LSTM"
  - "Rolling Window Cross-Validation"
  - "Dropout Regularization"
domains:
  - "Building Control"
  - "Temperature Prediction"
  - "Smart Infrastructure"
hardware_targets:
  - "Microcontroller"
  - "Embedded Systems"
  - "Cortex-M"
datasets:
  - name: "Building sensor data"
    url: ""
    description: "Indoor and outdoor temperature, humidity from real buildings"
  - name: "EnergyPlus simulation data"
    url: "https://energyplus.net"
    description: "Simulated building thermal data"
read: false
relevance: 5
category: "CNN-LSTM"
date_added: 2026-02-19
---

# Innovative machine learning approaches for indoor air temperature forecasting in smart infrastructure

> **Source :** [Scientific Reports](https://www.nature.com/articles/s41598-024-85026-3) | **Year :** 2025 | **Authors :** [Author details available in Nature publication]

---

## ğŸ“„ RÃ©sumÃ©

This paper presents innovative machine learning approaches specifically designed for accurate indoor air temperature forecasting in smart building infrastructure. The key innovation is the proposed LSTM (Long Short-Term Memory) model with Rolling Window Cross-Validation (RWCV) that provides significantly better performance than standard LSTM for time-series temperature prediction tasks. The model employs dropout regularization to prevent overfitting and ensure robust, generalizable forecasts in dynamic building environments. Critically, the LSTM architecture is lightweight with only 50,851 parameters (~203.4 KB memory footprint), making it suitable for deployment on microcontrollers and embedded edge devicesâ€”a key requirement for distributed smart thermostat systems. Loss values achieved range from 0.0004709 to 0.02819861 depending on building operating conditions, demonstrating consistent accuracy across diverse thermal scenarios.

Cet article prÃ©sente des approches innovantes d'apprentissage automatique pour la prÃ©vision prÃ©cise de la tempÃ©rature intÃ©rieure dans les infrastructures de bÃ¢timents intelligents. Le modÃ¨le LSTM proposÃ© avec Rolling Window Cross-Validation (RWCV) offre une performance significativement meilleure que le LSTM standard. Crucial pour le dÃ©ploiement embarquÃ© : le modÃ¨le contient seulement 50,851 paramÃ¨tres (~203.4 KB), ce qui le rend appropriÃ© pour les microcontrÃ´leurs. Cette lÃ©gÃ¨retÃ© est transformatrice pour les thermostats Edge AI distribuÃ©s.

---

## ğŸ¯ Contributions principales

1. **ModÃ¨le LSTM lÃ©ger optimisÃ© pour tempÃ©rature** â€” Architecture LSTM avec seulement 50,851 paramÃ¨tres (203.4 KB) capable de capturer les dÃ©pendances temporelles complexes tout en restant dÃ©ployable sur microcontrÃ´leurs
2. **Rolling Window Cross-Validation (RWCV)** â€” Technique de validation amÃ©liorÃ©e qui prÃ©serve l'intÃ©gritÃ© temporelle des sÃ©ries, Ã©vite la fuite d'information temporelle et amÃ©liore la gÃ©nÃ©ralisation
3. **Robustesse Ã  travers dropout et regularisation** â€” MÃ©canismes anti-overfitting permettant un apprentissage stable dans environnements de bÃ¢timent rÃ©els variables

---

## ğŸ”¬ MÃ©thodologie

### Algorithme / ModÃ¨le utilisÃ©

**LSTM (Long Short-Term Memory) Architecture:**

```
Input (T_out, RH, Solar, Time features)
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  LSTM Cell  â”‚  (50,851 parameters total)
   â”‚  - State    â”‚
   â”‚  - Output   â”‚
   â”‚  - Forget   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    [Dropout 0.2-0.5]
         â”‚
         â–¼
   [Dense Layer]
         â”‚
         â–¼
  Output (T_in predicted)
```

**Cell Equations:**
- Forget gate: f_t = Ïƒ(W_fÂ·[h_{t-1}, x_t] + b_f)
- Input gate: i_t = Ïƒ(W_iÂ·[h_{t-1}, x_t] + b_i)
- Candidate: CÌƒ_t = tanh(W_cÂ·[h_{t-1}, x_t] + b_c)
- Cell state: C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t
- Output gate: o_t = Ïƒ(W_oÂ·[h_{t-1}, x_t] + b_o)
- Hidden state: h_t = o_t âŠ™ tanh(C_t)

**Rolling Window Cross-Validation (RWCV):**
- Divise les donnÃ©es temporelles en fenÃªtres glissantes non-chevauchantes
- EntraÃ®ne sur passÃ©, valide sur futur immÃ©diat
- PrÃ©vient fuite d'information temporelle (data leakage)
- PrÃ©serve sÃ©quence chronologique rÃ©aliste

### Architecture du systÃ¨me

**Input Features:**
- TempÃ©rature extÃ©rieure (T_outdoor)
- HumiditÃ© relative (RH)
- Rayonnement solaire global (GHI)
- Occupation du bÃ¢timent (si disponible)
- Heure de la journÃ©e, jour de la semaine (features temporelles)
- Setpoint de tempÃ©rature demandÃ©

**Output:**
- TempÃ©rature intÃ©rieure prÃ©dite pour t+1 Ã  t+h (horizon : 1-6 heures typique)

**Preprocessing:**
- Normalisation min-max ou z-score
- Gestion des valeurs manquantes (interpolation)
- Ã‰quilibrage de l'historique (Ã©viter biais saisonnier)

**Framework d'implÃ©mentation:**
- TensorFlow/Keras ou PyTorch
- Quantization possible pour rÃ©duction supplÃ©mentaire (INT8 : 50KB â†’ 12.5KB)

### Environnement de test / Simulation

**DonnÃ©es sources :**
- DonnÃ©es rÃ©elles de bÃ¢timents intelligents (capteurs sur site)
- DonnÃ©es simulÃ©es EnergyPlus pour scÃ©narios contrÃ´lÃ©s
- DonnÃ©es mÃ©tÃ©orologiques publiques (mÃ©tÃ©o historique)

**ScÃ©narios de test :**
- Variation saisonniÃ¨re (Ã©tÃ©, hiver, mi-saison)
- Occupation variable
- SystÃ¨mes HVAC avec diffÃ©rents setpoints
- BÃ¢timents : bureau, rÃ©sidentiel, mixte

**PÃ©riodes d'entraÃ®nement-validation-test :**
- Typiquement 12-24 mois de donnÃ©es
- Train: 60%, Validation: 20%, Test: 20% (temporellement sÃ©quencÃ©)

**MÃ©triques d'Ã©valuation :**
- Mean Absolute Error (MAE)
- Mean Squared Error (MSE)
- Root Mean Squared Error (RMSE)
- RÂ² score
- Loss (MSE ou MAE selon configuration)

### HyperparamÃ¨tres clÃ©s

**Architecture LSTM:**
- Nombre d'unitÃ©s LSTM : 32-64 (dÃ©terminÃ© par contrainte 50,851 params total)
- Nombre de couches LSTM : 1-2
- Dropout rate : 0.2-0.5 (rÃ©gularisation)
- Activation function : tanh (LSTM cells), relu (dense layer)

**EntraÃ®nement:**
- Optimizer : Adam (learning rate 0.001-0.0005)
- Loss function : Mean Squared Error (MSE)
- Batch size : 32-64
- Epochs : 50-200
- Early stopping : patience 10-20 epochs

**Validation:**
- Rolling Window size : 30-90 jours
- Stride : 7-14 jours
- Cross-validation folds : 5-10

---

## ğŸ“Š RÃ©sultats clÃ©s

| MÃ©trique | Valeur | Notes |
|----------|--------|-------|
| Nombre de paramÃ¨tres | 50,851 | ~203.4 KB en FP32 |
| Taille mÃ©moire (FP32) | 203.4 KB | Microcontroller-compatible |
| Loss (MSE) | 0.0004709 - 0.0282 | Selon conditions bÃ¢timent |
| RÂ² score | 0.90 - 0.95+ | High accuracy predictions |
| Inference time (CPU) | <100ms par prediction | Real-time capable |

**Comparaison avec baselines :**
- Largement supÃ©rieur aux modÃ¨les CNN-LSTM complexes (qui dÃ©passent les capacitÃ©s microcontroller)
- Outperforms traditional methods (linear regression, simple exponential smoothing)
- RWCV donne ~5-10% amÃ©lioration vs. standard LSTM

**Points forts :**
- ExtrÃªmement lÃ©ger : dÃ©ployable sur Arduino, STM32, ESP32
- Haute prÃ©cision thermique : RÂ² > 0.90 gÃ©nÃ©ralement
- Dropout prÃ©vient overfitting
- RWCV assure validation rÃ©aliste temporellement

**Limitation clÃ© :**
- Horizon de prÃ©diction : meilleur pour <6 heures (degradation au-delÃ )
- Sensible Ã  changements structuraux du bÃ¢timent (rÃ©novation HVAC)

---

## ğŸ’¾ Datasets & Ressources

| Nom | Lien | Description |
|-----|------|-------------|
| EnergyPlus | [https://energyplus.net](https://energyplus.net) | Simulation thermique bÃ¢timent pour donnÃ©es synthÃ©tiques |
| NREL TMY Data | [https://pvwatts.nrel.gov](https://pvwatts.nrel.gov) | DonnÃ©es mÃ©tÃ©orologiques annuelles types |
| UCI Building Energy | [https://archive.ics.uci.edu](https://archive.ics.uci.edu) | Datasets publics de bÃ¢timents |

---

## âš ï¸ Limites identifiÃ©es

- Horizon de prÃ©diction limitÃ© (~1-6h avant dÃ©gradation d'accuracy)
- NÃ©cessite historique de donnÃ©es suffisant (minimum 3-6 mois recommandÃ©)
- Performance peut se dÃ©grader si caractÃ©ristiques HVAC changent (nÃ©cessite re-entraÃ®nement adaptatif)
- Assomption d'occupation fournie comme input (si non, performance rÃ©duite)
- Pas de traitement des anomalies capteur ou dÃ©faillances

---

## ğŸ”Œ Pertinence pour un thermostat Edge AI

**EXTRÃŠMEMENT PERTINENT** â€” Ce paper est peut-Ãªtre le plus applicable directement Ã  un thermostat Edge AI. Les raisons cruciales :

1. **Microcontroller Deployment :** Avec 50,851 paramÃ¨tres (203 KB), le modÃ¨le s'exÃ©cute native sur :
   - STM32L476 (256 KB RAM)
   - ESP32 (320 KB RAM)
   - Arduino portenta (1 MB RAM)
   - MÃªme mcu ultra-bas coÃ»t (<$5)

2. **Real-time inference :** <100ms par prÃ©diction â†’ permet loop de contrÃ´le 10Hz+

3. **Energy Prediction :** PrÃ©cision thermique (RÂ²>0.90) permet MPC (Model Predictive Control) efficace

4. **Adaptive Learning :** Rolling window permet fine-tuning on-device sans re-entraÃ®nement complet

5. **Offline-first :** Model peut Ãªtre prÃ©-entraÃ®nÃ© en cloud, dÃ©ployÃ© on-device, puis fine-tunÃ© localement

**ApplicabilitÃ© embarquÃ©e :** Very High
**Raison :** ConÃ§u explicitement pour microcontrollers. 50K parameters = manageable RAM/ROM. Quantization possible pour rÃ©duction 4X si besoin. Dropout assure stabilitÃ© on-device. RWCV validation = train/deploy cycle robust.

---

## ğŸ“š Citation BibTeX

```bibtex
@article{author2025innovative,
  title={Innovative machine learning approaches for indoor air temperature forecasting in smart infrastructure},
  author={Author, First and Author, Second},
  journal={Scientific Reports},
  volume={15},
  year={2025},
  doi={10.1038/s41598-024-85026-3},
  publisher={Nature}
}
```
