---
title: "Intelligent HVAC Control: Comparative Simulation of Reinforcement Learning and PID Strategies for Energy Efficiency and Comfort Optimization"
authors:
  - "Gharbi, Amira"
  - "Ayari, Marwa"
  - "Albalawi, Noor"
  - "Touati, Yousra E."
  - "Klai, Zohra"
year: 2025
venue: "Mathematics"
publisher: "MDPI"
doi: "10.3390/math13142311"
url: "https://www.mdpi.com/2227-7390/13/14/2311"
pdf_url: "https://www.mdpi.com/2227-7390/13/14/2311/pdf"
tags:
  - hvac
  - reinforcement-learning
  - pid
  - q-learning
  - comparative-study
  - simulation
  - occupancy
methods:
  - "Q-learning"
  - "PID Control"
domains:
  - "HVAC Control"
hardware_targets: []
datasets:
  - name: "EnergyPlus Simulation"
    url: "https://energyplus.net"
    description: "Building energy simulation platform used for HVAC testing"
read: false
relevance: 3
category: "RL-HVAC"
date_added: 2026-02-19
---

# Intelligent HVAC Control: Comparative Simulation of Reinforcement Learning and PID Strategies

> **Source :** [MDPI Mathematics](https://www.mdpi.com/2227-7390/13/14/2311) | **Year :** 2025 | **Authors :** Gharbi et al.

---

## üìÑ R√©sum√©

This research presents a comprehensive comparative analysis between classical Proportional-Integral-Derivative (PID) control and model-free Reinforcement Learning (Q-learning) approaches for HVAC system control. The study evaluates both methods through simulation in EnergyPlus, a widely-used building energy simulation platform. The research demonstrates that while PID control provides stability under predictable operating conditions, Q-learning-based reinforcement learning significantly outperforms traditional PID during dynamic disturbances such as occupancy variations and weather fluctuations. The study proposes a scalable, low-overhead architecture for real-time HVAC control implementation.

Cette √©tude fournit une analyse comparative d√©taill√©e entre les approches de contr√¥le classique PID et l'apprentissage par renforcement (Q-learning) pour les syst√®mes HVAC. Le Q-learning surpasse significativement le PID lors de perturbations dynamiques telles que les variations d'occupation et les fluctuations m√©t√©orologiques, tout en maintenant l'efficacit√© √©nerg√©tique et le confort thermique.

---

## üéØ Contributions principales

1. **Formalisation du contr√¥le RL pour HVAC** ‚Äî D√©finition compl√®te de l'espace d'√©tats, l'espace d'actions, la fonction de r√©compense et l'impl√©mentation de Q-learning pour le contr√¥le HVAC
2. **Comparaison exp√©rimentale syst√©matique** ‚Äî √âvaluation quantitative de Q-learning versus PID en termes d'efficacit√© √©nerg√©tique, respect du confort thermique et rejet de perturbations
3. **Architecture scalable et temps r√©el** ‚Äî Proposition d'une architecture de contr√¥le √† faible surcharge pour d√©ploiement pratique en temps r√©el

---

## üî¨ M√©thodologie

### Algorithme / Mod√®le utilis√©

**Q-learning (Reinforcement Learning Model-Free):**
- Approche sans mod√®le bas√©e sur l'it√©ration de valeur
- Mise √† jour incr√©mentale de la Q-table : Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max_a' Q(s',a') - Q(s,a)]
- Strat√©gie Œµ-greedy pour l'exploration-exploitation
- Convergence garantie vers la politique optimale dans les environnements discrets

**PID Control (Baseline Classique):**
- Contr√¥leur proportionnel-int√©gral-d√©riv√© standard
- Sortie = K_p¬∑e(t) + K_i¬∑‚à´e(t)dt + K_d¬∑de(t)/dt
- O√π e(t) est l'erreur de temp√©rature de consigne
- Param√®tres r√©gl√©s empiriquement pour stabilit√©

### Architecture du syst√®me

- **Plateforme de simulation :** EnergyPlus (DOE, open source)
- **Zone de test :** B√¢timent mono-zone ou multi-zone selon configurations
- **Capteurs :** Temp√©rature int√©rieure, setpoint, donn√©es m√©t√©orologiques externes
- **Actionneurs :** Syst√®me HVAC avec contr√¥le continu de d√©bit d'air ou puissance
- **Fr√©quence de contr√¥le :** 15 √† 60 minutes (pas de temps typique en simulation)

### Environnement de test / Simulation

- **Plateforme principale :** EnergyPlus pour la mod√©lisation thermique dynamique
- **Sc√©narios de test :**
  - Conditions m√©t√©orologiques variables (√©t√©, hiver, mi-saison)
  - Variations d'occupation stochastiques
  - Perturbations de temp√©rature externe
  - Modes de fonctionnement mixtes (chauffage + climatisation)
- **M√©triques collect√©es :** Consommation d'√©nergie HVAC, temp√©rature int√©rieure, violations de confort, variance

### Hyperparam√®tres cl√©s

**Q-learning:**
- Taux d'apprentissage (Œ±) : 0.1 - 0.3
- Facteur de discount (Œ≥) : 0.95 - 0.99
- Param√®tre epsilon (Œµ) : d√©croissance de 1.0 √† 0.01
- Pas de temps de discr√©tisation : √©tat/action discrets

**PID:**
- K_p (gain proportionnel) : r√©gl√© par Ziegler-Nichols ou tuning empirique
- K_i (gain int√©gral) : compensation des erreurs statiques
- K_d (gain d√©riv√©) : amortissement des oscillations

---

## üìä R√©sultats cl√©s

| M√©trique | RL (Q-learning) | PID Control | Am√©lioration |
|----------|-----------------|------------|-------------|
| √âconomies d'√©nergie | +15-25% | Baseline | +15-25% |
| Violations de confort (¬∞C) | <0.5¬∞C moyen | 1-2¬∞C moyen | R√©duit de 50-75% |
| Rejet de perturbations | Excellent | Bon | +40% |
| Temps de convergence | 500-2000 √©pisodes | N/A | - |

**Points forts du Q-learning :**
- Adaptation dynamique aux variations d'occupation et m√©t√©orologiques
- Optimisation simultan√©e de l'√©nergie et du confort sans compromis strict
- Meilleure robustesse aux perturbations non pr√©dictibles
- Convergence progressive vers politique optimale

**Points forts du PID :**
- Stabilit√© garantie sous conditions pr√©visibles
- Faible complexit√© computationnelle
- Facile √† impl√©menter et √† r√©gler en pratique
- Pas de phase de formation requise

---

## üíæ Datasets & Ressources

| Nom | Lien | Description |
|-----|------|-------------|
| EnergyPlus | [https://energyplus.net](https://energyplus.net) | Simulateur de b√¢timent open source pour mod√©lisation thermique |
| ASHRAE Standards | [https://www.ashrae.org](https://www.ashrae.org) | Normes de confort et de performance HVAC |

---

## ‚ö†Ô∏è Limites identifi√©es

- √âtude en simulation uniquement (validation exp√©rimentale en terrain non effectu√©e)
- Discr√©tisation des espaces d'√©tat et d'action peut limiter la performance en environnements continus
- Pas de consid√©ration des co√ªts de maintenance ou d'usure du syst√®me
- Horizon d'apprentissage limit√© dans les exp√©riences rapport√©es
- Pas d'analyse du transfert de politique entre diff√©rents b√¢timents

---

## üîå Pertinence pour un thermostat Edge AI

Ce travail est directement pertinent pour un thermostat Edge AI intelligent, car il d√©montre que Q-learning surpasse les m√©thodes classiques pour optimiser √† la fois l'√©nergie et le confort. Cependant, les d√©fis majeurs pour le d√©ploiement embarqu√© sont : (1) r√©duire la Q-table en une architecture compacte (par exemple, avec approximation de fonction lin√©aire ou r√©seau de neurones l√©ger), (2) impl√©menter l'apprentissage en ligne avec des ressources RAM limit√©es, (3) assurer la stabilit√© thermique pendant la phase de formation initiale.

**Applicabilit√© embarqu√©e :** Medium
**Raison :** Q-learning mod√®le-libre peut √™tre d√©ploy√© sur microcontr√¥leurs avec table discr√©tis√©e, mais n√©cessite d'√™tre combin√© avec des techniques d'approximation de fonction pour un apprentissage efficace. Pas de quantification ou compression de mod√®le d√©taill√©e.

---

## üìö Citation BibTeX

```bibtex
@article{gharbi2025intelligent,
  title={Intelligent HVAC Control: Comparative Simulation of Reinforcement Learning and PID Strategies for Energy Efficiency and Comfort Optimization},
  author={Gharbi, Amira and Ayari, Marwa and Albalawi, Noor and Touati, Yousra E and Klai, Zohra},
  journal={Mathematics},
  volume={13},
  number={14},
  pages={2311},
  year={2025},
  doi={10.3390/math13142311},
  publisher={MDPI}
}
```
