---
title: "Energy-efficient heating control for smart buildings with deep reinforcement learning"
authors:
  - "Gupta, Anchal"
  - "Badr, Youakim"
  - "Negahban, Ashkan"
  - "Qiu, Robin G."
year: 2021
venue: "Journal of Building Engineering"
publisher: "Elsevier"
doi: "10.1016/j.jobe.2020.101739"
url: "https://www.sciencedirect.com/science/article/abs/pii/S2352710220333726"
pdf_url: null
tags:
  - hvac
  - dqn
  - deep-reinforcement-learning
  - heating
  - smart-building
  - thermal-comfort
  - energy-efficiency
domains:
  - "HVAC Control"
  - "Building Energy Management"
methods:
  - "Deep Q-Network (DQN)"
  - "Reinforcement Learning"
hardware_targets: []
datasets:
  - name: "EnergyPlus Simulation"
    url: "https://energyplus.net"
    description: "Building energy simulation environment"
read: false
relevance: 4
category: "RL-HVAC"
date_added: 2026-02-19
---

# Energy-efficient heating control for smart buildings with deep reinforcement learning

> **Source :** [Journal of Building Engineering - ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S2352710220333726) | **Ann√©e :** 2021 | **Auteurs :** Gupta, Badr, Negahban, Qiu

---

## üìÑ R√©sum√©

This paper presents a real-time decision-making algorithm for heating control in smart buildings using deep reinforcement learning. The approach employs a Deep Q-Network (DQN) controller to optimize both thermal comfort and energy consumption in building HVAC systems. The study demonstrates that DRL-based smart controllers can effectively balance conflicting objectives‚Äîmaintaining occupant comfort while minimizing energy usage‚Äîin ways that traditional thermostat-based controllers cannot achieve.

**R√©sum√© fran√ßais :** Ce travail propose un algorithme d'apprentissage par renforcement profond pour le contr√¥le du chauffage dans les b√¢timents intelligents. En utilisant une approche DQN, les chercheurs d√©veloppent un contr√¥leur capable d'optimiser simultan√©ment le confort thermique et la consommation d'√©nergie. L'√©tude montre que cette approche surpasse significativement les contr√¥leurs traditionnels en termes de confort des occupants et d'efficacit√© √©nerg√©tique.

---

## üéØ Contributions principales

1. **Algorithme de contr√¥le DQN pour le chauffage** ‚Äî Pr√©sentation d'un contr√¥leur en temps r√©el bas√© sur DQN pour optimiser le chauffage des b√¢timents intelligents, capable de g√©rer les compromis entre confort thermique et consommation √©nerg√©tique.

2. **Am√©lioration du confort thermique** ‚Äî D√©monstration d'une am√©lioration de 15-30% du confort thermique (r√©duction de l'√©cart avec la temp√©rature de consigne) compar√© aux thermostats traditionnels.

3. **R√©duction de la consommation √©nerg√©tique** ‚Äî Atteinte d'une r√©duction de 5-12% de la consommation d'√©nergie de chauffage tout en maintenant ou am√©liorant le confort des occupants.

4. **Analyse de contr√¥le centralis√© vs d√©centralis√©** ‚Äî √âtude comparative des strat√©gies de contr√¥le pour plusieurs b√¢timents, montrant les avantages et limitations de chaque approche selon la configuration et le nombre de b√¢timents.

---

## üî¨ M√©thodologie

### Algorithme / Mod√®le utilis√©

Le papier utilise un **Deep Q-Network (DQN)**, un algorithme d'apprentissage par renforcement profond qui estime les valeurs optimales d'action (Q-values) pour chaque √©tat du syst√®me. Le DQN combine:

- **R√©seau de neurones profond** : Approche fonction-approximation pour estimer Q(s,a)
- **Experience replay** : Stockage et r√©√©chantillonnage des transitions pour am√©liorer la stabilit√© de l'apprentissage
- **Target network** : R√©seau s√©par√© pour le calcul des cibles Q, r√©duisant les divergences

### Architecture du syst√®me

Le syst√®me de contr√¥le op√®re dans un cycle d'apprentissage et d√©cision :

1. **√âtat du syst√®me** : Temp√©rature int√©rieure actuelle, temp√©rature ext√©rieure, historique des temp√©ratures, pr√©sence occupants
2. **Espace d'actions** : Ajustements discrets du point de consigne de chauffage ou puissance de chauffage (contr√¥le multi-niveaux)
3. **Fonction de r√©compense** : Combinaison pond√©r√©e du confort thermique (√©cart minimal par rapport √† la consigne) et de la consommation √©nerg√©tique (minimisation)
4. **Processus d'apprentissage** : Interaction it√©rative avec l'environnement de simulation pour affiner la politique de contr√¥le

### Environnement de test / Simulation

- **Plateforme de simulation** : EnergyPlus, un simulateur de performance √©nerg√©tique des b√¢timents largement utilis√©
- **Configuration exp√©rimentale** : Mod√®les de b√¢timents simples √† complexes avec diff√©rentes configurations de chauffage
- **Donn√©es m√©t√©orologiques** : Profils typiques de temp√©rature ext√©rieure pour diff√©rents climats
- **Dur√©e d'apprentissage** : Plusieurs p√©riodes de chauffage (jours/semaines) de simulation pour permettre la convergence du DQN

### Hyperparam√®tres cl√©s

- **Taille du r√©seau Q** : Architecture multicouche (d√©tails sp√©cifiques non pleinement disponibles)
- **Taux d'apprentissage** : Optimis√© pour convergence stable du DQN
- **Facteur de discount (gamma)** : Typiquement 0.99 pour √©quilibrer les r√©compenses imm√©diates et futures
- **Epsilon-greedy exploration** : D√©croissance progressive de l'exploration pour exploitation progressive

---

## üìä R√©sultats cl√©s

| M√©trique | R√©sultat | R√©f√©rence compar√©e |
|----------|----------|-------------------|
| Am√©lioration confort thermique | 15-30% | Thermostat traditionnel |
| R√©duction consommation √©nerg√©tique | 5-12% | Thermostat traditionnel |
| Gestion multi-b√¢timents | Meilleure avec d√©centralisation | Contr√¥le centralis√© |

**Points forts :**
- Le contr√¥leur DQN optimise simultan√©ment deux objectifs conflictuels (confort vs √©nergie)
- Am√©lioration substantielle du confort sans sacrifice √©nerg√©tique majeur
- Approche adapt√©e au contr√¥le en temps r√©el dans b√¢timents complexes
- Montre l'applicabilit√© de DRL √† des probl√®mes r√©els de gestion √©nerg√©tique

**Limitations observ√©es :**
- Les r√©sultats sont bas√©s sur simulation (EnergyPlus) plut√¥t que d√©ploiement r√©el
- Performance d√©pendante de la qualit√© et compl√©tude des √©tats observ√©s
- Convergence du DQN peut n√©cessiter temps d'apprentissage significatif en pratique

---

## üíæ Datasets & Ressources

| Nom | Lien | Description |
|-----|------|-------------|
| EnergyPlus | [https://energyplus.net](https://energyplus.net) | Simulateur de performance √©nerg√©tique des b√¢timents, standard pour validation |

---

## ‚ö†Ô∏è Limites identifi√©es

- **Validation simulation uniquement** : Les r√©sultats proviennent de simulations EnergyPlus, pas de d√©ploiements r√©els avec donn√©es de b√¢timents r√©els
- **√âtat d'observation limit√©** : Performance d√©pend de la qualit√© de l'observation compl√®te des √©tats (temp√©rature, conditions m√©t√©o)
- **Temps d'entra√Ænement** : Convergence du DQN peut n√©cessiter p√©riodes d'apprentissage √©tendues
- **Scalabilit√©** : Approche centralis√©e complexe pour tr√®s grands b√¢timents ou r√©seaux multi-b√¢timents
- **Co√ªts de calcul** : Entra√Ænement et inf√©rence du DQN plus co√ªteux que contr√¥le traditionnel

---

## üîå Pertinence pour un thermostat Edge AI

Ce papier est hautement pertinent pour le design d'un thermostat Edge AI car il d√©montre comment les techniques d'apprentissage par renforcement profond peuvent optimiser simultan√©ment confort et efficacit√© √©nerg√©tique‚Äîdeux objectifs critiques pour tout syst√®me de thermostat intelligent.

L'approche DQN offre un bon √©quilibre entre performance et complexit√© computationnelle. Les r√©sultats montrent que m√™me avec une optimisation relativement simple (DQN vs algorithmes plus avanc√©s), on peut obtenir des gains significatifs. L'utilisation d'EnergyPlus pour la validation fournit un benchmark standard dans l'industrie.

Cependant, pour un d√©ploiement Edge AI, il faudrait consid√©rer :
- **R√©duction du mod√®le** : Quantification ou distillation du DQN pour ex√©cution sur mat√©riel embarqu√©
- **Apprentissage en ligne** : Adaptation continue aux patterns thermiques sp√©cifiques du b√¢timent
- **Observation partielle** : Gestion des √©tats partiellement observables typiques dans les thermostats r√©els

**Applicabilit√© embarqu√©e :** Medium
**Raison :** Le DQN est relativement simple compar√© √† des approches plus avanc√©es (Actor-Critic, PPO), ce qui le rend candidat pour optimisation embarqu√©e. Cependant, les temps d'entra√Ænement et les besoins m√©moire devront √™tre valid√©s pour les contraintes d'un thermostat edge.

---

## üìö Citation BibTeX

```bibtex
@article{Gupta2021,
  title = {Energy-efficient heating control for smart buildings with deep reinforcement learning},
  author = {Gupta, Anchal and Badr, Youakim and Negahban, Ashkan and Qiu, Robin G.},
  journal = {Journal of Building Engineering},
  year = {2021},
  volume = {34},
  pages = {101739},
  doi = {10.1016/j.jobe.2020.101739},
  publisher = {Elsevier}
}
```
